{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making your data accessible to the GIS\n",
    "Collecting, storing, managing and analyzing large quantities of numbers, figures, and files is not a new business activity. But referring to these numbers, figures and files as big data is relatively recent. Analysts, researchers and other professionals popularly characterize big data as sharing 4 key traits, also known as the 4 Vs:\n",
    "\n",
    " - high **volume**: a large quantity of data that cannot be efficiently managed by traditional relational databases or analyzed in a traditional manner using the memory of a single machine \n",
    " - high **velocity**: data that is dynamic and streams in at a fast pace from multiple sources \n",
    " - large **variety**: different formats from structured to unstructured; tabular to documents, email, video or audio; spatial to non-spatial; \n",
    " - unknown **veracity**: data that is unprocessed, uncleaned, inconsistent or unscreened and of unknown origin or quality\n",
    " \n",
    "The GeoAnalytics Server expands your ArcGIS Enterprise deployment providing functionality and services to process and analyze big data.\n",
    "\n",
    "## Big data file shares\n",
    "The GeoAnalytics server allows you to register datasets in a format called a [big data file share](http://enterprise.arcgis.com/en/server/latest/get-started/windows/what-is-a-big-data-file-share.htm). Big data file shares are items on your Web GIS, and can reference data in any of the following data sources:\n",
    " - File Share - a directory of datasets stored locally or shared across a network\n",
    " - HDFS - an [Hadoop Distributed File System](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html#Introduction) directory of datasets\n",
    " - [Apache Hive](https://hive.apache.org/) - a metastore database\n",
    " - Cloud Store - an [Azure Blob Storage](https://azure.microsoft.com/en-us/services/storage/blobs/) container or [Amazon Web Services S3 bucket](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingBucket.html) \n",
    "\n",
    "Storing your data in a big data file share datastore benefits you because:\n",
    " - The GeoAnalytics tools read your data only when they are executed, which allows you to update or add data to these locations.\n",
    " - You can use partitioned data as a single dataset.\n",
    " - Big data file shares are flexible in how time and geometry are defined, allowing data in multiple formats in a single dataset.\n",
    "\n",
    "### Preparing your data\n",
    "To register a file share or an HDFS, you need to format your datasets as subfolders within a single parent folder and register the parent folder. This parent folder becomes a `datastore`, and each subfolder becomes a `dataset`. For instance, to register 2 datasets representing earthquakes and hurricanes, your folder hierarchy would look like below:\n",
    "```\n",
    "|---FileShareFolder         <-- register as a datastore\n",
    "   |---Earthquakes          <-- dataset 1\n",
    "      |---1960              \n",
    "         |---01_1960.csv\n",
    "         |---02_1960.csv\n",
    "      |---1961              \n",
    "         |---01_1961.csv\n",
    "         |---02_1961.csv\n",
    "   |---Hurricanes           <-- dataset 2\n",
    "      |---atlantic_hur.shp\n",
    "      |---pacific_hur.shp\n",
    "```\n",
    "Learn more about preparing your big data file share datasets [here](http://server.arcgis.com/en/server/latest/get-started/windows/what-is-a-big-data-file-share.htm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Connect to enterprise GIS\n",
    "from arcgis.gis import GIS\n",
    "import arcgis.geoanalytics\n",
    "portal_gis = GIS(\"portal_url\", \"username\", \"password\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensuring your GIS supports GeoAnalytics\n",
    "It is best practice to confirm proper configuration of your Enterprise to support the GeoAnalytics Server. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that GeoAnalytics is supported \n",
    "arcgis.geoanalytics.is_supported()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for big data file shares\n",
    "The [`get_datastores()`](https://esri.github.io/arcgis-python-api/apidoc/html/arcgis.geoanalytics.toc.html#get-datastores) method of the `geoanalytics` module returns a [`DatastoreManager`](https://esri.github.io/arcgis-python-api/apidoc/html/arcgis.gis.toc.html#datastoremanager) object that lets you search for and manage the big data file share items as Python API  [`Datastore`](https://esri.github.io/arcgis-python-api/apidoc/html/arcgis.gis.toc.html#datastore) objects on your GeoAnalytics server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatastoreManager for https://dev0001561.esri.com/gax/admin>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigdata_datastore_manager = arcgis.geoanalytics.get_datastores()\n",
    "bigdata_datastore_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the [`search()`](https://esri.github.io/arcgis-python-api/apidoc/html/arcgis.gis.toc.html#arcgis.gis.DatastoreManager.search) method on a `DatastoreManager` object to search for `Datastores`. Observe in the output below the item titled _FileShareFolder_ as illustrated in the example file structure above is registered as a big data file share in the portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Datastore title:\"/bigDataFileShares/FileShareFolder\" type:\"bigDataFileShare\">,\n",
       " <Datastore title:\"/bigDataFileShares/hdfs_test\" type:\"bigDataFileShare\">,\n",
       " <Datastore title:\"/bigDataFileShares/qalab\" type:\"bigDataFileShare\">]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigdata_fileshares = bigdata_datastore_manager.search()\n",
    "bigdata_fileshares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get datasets from a big data file share datastore\n",
    "Let's use the `datasets` property on a `Datastore` object to find out how many datasets are available and then list them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_share_folder = bigdata_fileshares[0]\n",
    "file_share_datasets = file_share_folder.datasets\n",
    "len(file_share_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 0:   Earthquakes\n",
      "Dataset 1:   Hurricanes\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(file_share_datasets)):\n",
    "    print(\"{:<10}{:<3}{}\".format(\"Dataset \" + str(i) + \":\", \"\", file_share_datasets[i]['name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'format': {'extension': 'shp', 'type': 'shapefile'},\n",
       " 'geometry': {'geometryType': 'esriGeometryPoint',\n",
       "  'spatialReference': {'wkid': 4326}},\n",
       " 'name': 'Hurricanes',\n",
       " 'schema': {'fields': [{'name': 'serial_num', 'type': 'esriFieldTypeString'},\n",
       "   {'name': 'season', 'type': 'esriFieldTypeBigInteger'},\n",
       "   {'name': 'num', 'type': 'esriFieldTypeBigInteger'},\n",
       "   {'name': 'basin', 'type': 'esriFieldTypeString'},\n",
       "   {'name': 'sub_basin', 'type': 'esriFieldTypeString'},\n",
       "   {'name': 'name', 'type': 'esriFieldTypeString'},\n",
       "   {'name': 'iso_time', 'type': 'esriFieldTypeString'},\n",
       "   {'name': 'nature', 'type': 'esriFieldTypeString'},\n",
       "   {'name': 'latitude', 'type': 'esriFieldTypeDouble'},\n",
       "   {'name': 'longitude', 'type': 'esriFieldTypeDouble'},\n",
       "   {'name': 'wind_wmo_', 'type': 'esriFieldTypeDouble'},\n",
       "   {'name': 'pres_wmo_', 'type': 'esriFieldTypeBigInteger'},\n",
       "   {'name': 'center', 'type': 'esriFieldTypeString'},\n",
       "   {'name': 'wind_wmo1', 'type': 'esriFieldTypeDouble'},\n",
       "   {'name': 'pres_wmo1', 'type': 'esriFieldTypeDouble'},\n",
       "   {'name': 'track_type', 'type': 'esriFieldTypeString'},\n",
       "   {'name': 'size', 'type': 'esriFieldTypeString'},\n",
       "   {'name': 'Wind', 'type': 'esriFieldTypeBigInteger'}]},\n",
       " 'time': {'fields': [{'formats': ['yyyy-MM-dd HH:mm:ss'], 'name': 'iso_time'}],\n",
       "  'timeReference': {'timeZone': 'UTC'},\n",
       "  'timeType': 'instant'}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's view the json schema of the hurricanes dataset for a sample\n",
    "file_share_datasets[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering big data file shares\n",
    "You can register your data as a big data file share using the `add_bigdata()` method on a `DatastoreManager` object. Ensure the datasets are stored in a format compatible with the GeoAnalytics server as seen earlier in this guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Big Data file share for Sample_US_City_Crime\n"
     ]
    }
   ],
   "source": [
    "Sample_City_Crime_data_item = bigdata_datastore_manager.add_bigdata(\"Sample_US_City_Crime\", \n",
    "                                                      r\"\\\\<file_share_path>\\<big_data_folder>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Datastore title:\"/bigDataFileShares/Sample_US_City_Crime\" type:\"bigDataFileShare\">"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sample_City_Crime_data_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a big data file share is created, the GeoAnalytics server samples the datasets to generate the schema of the data to create a manifest. This process can take a few minutes depending on the size of your data. Once processed, querying the `manifest` property returns the schema of the datasets in your big data file share."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'datasets': [{'format': {'encoding': 'UTF-8',\n",
       "    'extension': 'csv',\n",
       "    'fieldDelimiter': ',',\n",
       "    'hasHeaderRow': True,\n",
       "    'quoteChar': '\"',\n",
       "    'recordTerminator': '\\n',\n",
       "    'type': 'delimited'},\n",
       "   'name': 'HoustonCrime',\n",
       "   'schema': {'fields': [{'name': 'Address', 'type': 'esriFieldTypeString'},\n",
       "     {'name': 'Beat', 'type': 'esriFieldTypeString'},\n",
       "     {'name': 'BlockRange', 'type': 'esriFieldTypeString'},\n",
       "     {'name': 'Date', 'type': 'esriFieldTypeString'},\n",
       "     {'name': 'DayOfWeek', 'type': 'esriFieldTypeString'},\n",
       "     {'name': 'Hour', 'type': 'esriFieldTypeDouble'},\n",
       "     {'name': 'OffenseTyp', 'type': 'esriFieldTypeString'},\n",
       "     {'name': 'Offenses', 'type': 'esriFieldTypeDouble'},\n",
       "     {'name': 'Premise', 'type': 'esriFieldTypeString'},\n",
       "     {'name': 'StreetName', 'type': 'esriFieldTypeString'},\n",
       "     {'name': 'Suffix', 'type': 'esriFieldTypeString'},\n",
       "     {'name': 'Type', 'type': 'esriFieldTypeString'},\n",
       "     {'name': 'x', 'type': 'esriFieldTypeDouble'},\n",
       "     {'name': 'y', 'type': 'esriFieldTypeDouble'}]},\n",
       "   'time': {'fields': [{'formats': ['yyyy-MM-dd'], 'name': 'Date'}],\n",
       "    'timeReference': {'timeZone': 'UTC'},\n",
       "    'timeType': 'instant'}},\n",
       "  {'format': {'encoding': 'UTF-8',\n",
       "    'extension': 'csv',\n",
       "    'fieldDelimiter': ',',\n",
       "    'hasHeaderRow': True,\n",
       "    'quoteChar': '\"',\n",
       "    'recordTerminator': '\\n',\n",
       "    'type': 'delimited'},\n",
       "   'name': 'PhiladelphiaCrime',\n",
       "   'schema': {'fields': [{'name': 'X', 'type': 'esriFieldTypeDouble'},\n",
       "     {'name': 'Y', 'type': 'esriFieldTypeDouble'},\n",
       "     {'name': 'DC_DIST', 'type': 'esriFieldTypeBigInteger'},\n",
       "     {'name': 'SECTOR', 'type': 'esriFieldTypeBigInteger'},\n",
       "     {'name': 'DISPATCH_DATE_TIME', 'type': 'esriFieldTypeString'},\n",
       "     {'name': 'DISPATCH_DATE', 'type': 'esriFieldTypeString'},\n",
       "     {'name': 'DISPATCH_TIME', 'type': 'esriFieldTypeString'},\n",
       "     {'name': 'HOUR', 'type': 'esriFieldTypeString'},\n",
       "     {'name': 'DC_KEY', 'type': 'esriFieldTypeBigInteger'},\n",
       "     {'name': 'LOCATION_BLOCK', 'type': 'esriFieldTypeString'},\n",
       "     {'name': 'UCR_GENERAL', 'type': 'esriFieldTypeBigInteger'},\n",
       "     {'name': 'OBJECTID', 'type': 'esriFieldTypeBigInteger'},\n",
       "     {'name': 'TEXT_GENERAL_CODE', 'type': 'esriFieldTypeString'},\n",
       "     {'name': 'POINT_X', 'type': 'esriFieldTypeDouble'},\n",
       "     {'name': 'POINT_Y', 'type': 'esriFieldTypeDouble'},\n",
       "     {'name': 'GlobalID', 'type': 'esriFieldTypeString'}]},\n",
       "   'time': {'fields': [{'formats': [\"yyyy-MM-dd'T'HH:mm:ss.SSSZ\"],\n",
       "      'name': 'DISPATCH_DATE_TIME'}],\n",
       "    'timeReference': {'timeZone': 'UTC'},\n",
       "    'timeType': 'instant'}}]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sample_City_Crime_data_item.manifest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
